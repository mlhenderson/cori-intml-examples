{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed random-search hyper-parameter optimization of the Keras RPV classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "import os\n",
    "\n",
    "# External imports\n",
    "import ipyparallel as ipp\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local imports\n",
    "from rpv import load_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "squeue -u sfarrell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cluster ID taken from job ID above\n",
    "job_id = 24194075\n",
    "cluster_id = 'cori_{}'.format(job_id)\n",
    "\n",
    "# Use default profile\n",
    "c = ipp.Client(timeout=60, cluster_id=cluster_id)\n",
    "print('Worker IDs:', c.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the hyper-parameter search tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data config\n",
    "n_train = 64000 #412416\n",
    "n_valid = 32000 #137471\n",
    "n_test = 32000 #137471\n",
    "input_dir = '/global/cscratch1/sd/sfarrell/atlas-rpv-images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily making things reproducible for development\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the hyper-parameter search points\n",
    "n_hpo_trials = 32\n",
    "h1 = np.random.choice([4, 8, 16, 32, 64], size=n_hpo_trials)\n",
    "h2 = np.random.choice([4, 8, 16, 32, 64], size=n_hpo_trials)\n",
    "h3 = np.random.choice([8, 16, 32, 64, 128], size=n_hpo_trials)\n",
    "conv_sizes = np.stack([h1, h2, h3], axis=1)\n",
    "fc_sizes = np.random.choice([32, 64, 128, 256], size=(n_hpo_trials, 1))\n",
    "lr = np.random.choice([0.0001, 0.001, 0.01], size=n_hpo_trials)\n",
    "dropout = np.random.rand(n_hpo_trials)\n",
    "optimizer = np.random.choice(['Adadelta', 'Adam', 'Nadam'], size=n_hpo_trials)\n",
    "\n",
    "# Training config\n",
    "batch_size = 64\n",
    "n_epochs = 16\n",
    "checkpoint_dir = os.path.join(os.environ['SCRATCH'],\n",
    "                              'cori-interactive-dl/rpv_hpo_%i' % job_id)\n",
    "os.makedirs(checkpoint_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the hyper-parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(input_dir, n_train, n_valid,\n",
    "                    conv_sizes, fc_sizes, dropout, optimizer, lr,\n",
    "                    batch_size, n_epochs, checkpoint_file=None, verbose=2):\n",
    "    \"\"\"Run training for one set of hyper-parameters\"\"\"\n",
    "    import os\n",
    "    import keras\n",
    "    from rpv import build_model, train_model, load_file\n",
    "    from mlextras import configure_session\n",
    "    print('Hyperparameters: conv %s fc %s dropout %.3f opt %s, lr %.4f' %\n",
    "          (conv_sizes, fc_sizes, dropout, optimizer, lr))\n",
    "    # Load the dataset\n",
    "    train_input, train_labels, train_weights = load_file(os.path.join(input_dir, 'train.h5'), n_train)\n",
    "    valid_input, valid_labels, valid_weights = load_file(os.path.join(input_dir, 'val.h5'), n_valid)\n",
    "    print('train shape:', train_input.shape, 'Mean label:', train_labels.mean())\n",
    "    print('valid shape:', valid_input.shape, 'Mean label:', valid_labels.mean())\n",
    "    # Thread settings\n",
    "    keras.backend.set_session(configure_session())\n",
    "    # Build the model\n",
    "    model = build_model(train_input.shape[1:],\n",
    "                        conv_sizes=conv_sizes, fc_sizes=fc_sizes,\n",
    "                        dropout=dropout, optimizer=optimizer, lr=lr)\n",
    "    # Train the model\n",
    "    history = train_model(model, train_input=train_input, train_labels=train_labels,\n",
    "                          valid_input=valid_input, valid_labels=valid_labels,\n",
    "                          batch_size=batch_size, n_epochs=n_epochs,\n",
    "                          checkpoint_file=checkpoint_file, verbose=verbose)\n",
    "    return history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load-balanced view\n",
    "lv = c.load_balanced_view()\n",
    "\n",
    "# Loop over hyper-parameter sets\n",
    "results = []\n",
    "for ihp in range(n_hpo_trials):\n",
    "    print('Hyperparameter trial %i conv %s fc %s dropout %.4f opt %s, lr %.4f' %\n",
    "          (ihp, conv_sizes[ihp], fc_sizes[ihp], dropout[ihp], optimizer[ihp], lr[ihp]))\n",
    "    checkpoint_file = os.path.join(checkpoint_dir, 'model_%i.h5' % ihp)\n",
    "    result = lv.apply(build_and_train,\n",
    "                      input_dir, n_train, n_valid,\n",
    "                      conv_sizes=conv_sizes[ihp], fc_sizes=fc_sizes[ihp],\n",
    "                      dropout=dropout[ihp], optimizer=optimizer[ihp], lr=lr[ihp],\n",
    "                      batch_size=batch_size, n_epochs=n_epochs,\n",
    "                      checkpoint_file=checkpoint_file)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tasks completed: %i / %i' % (np.sum([ar.ready() for ar in results]), len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print standard out for one of the runs\n",
    "ar = results[1]\n",
    "print('STDOUT')\n",
    "print(ar.stdout)\n",
    "print('STDERR')\n",
    "print(ar.stderr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pull the results from all of the runs that have finished\n",
    "histories = [ar.get() for ar in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "times = [(ar.completed-ar.started).total_seconds()/60. for ar in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(times)\n",
    "plt.xlabel('Training time [min]');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_history(h):\n",
    "    plt.figure(figsize=(9,4))\n",
    "    # Loss\n",
    "    plt.subplot(121)\n",
    "    plt.plot(h['loss'], label='Training')\n",
    "    plt.plot(h['val_loss'], label='Validation')\n",
    "    plt.xlim(xmin=0, xmax=len(h['loss'])-1)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc=0)\n",
    "    # Accuracy\n",
    "    plt.subplot(122)\n",
    "    plt.plot(h['acc'], label='Training')\n",
    "    plt.plot(h['val_acc'], label='Validation')\n",
    "    plt.xlim(xmin=0, xmax=len(h['loss'])-1)\n",
    "    plt.ylim((0, 1))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc=0)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_scores = np.array([h['val_acc'][-1] for h in histories])\n",
    "best_scores = np.array([max(h['val_acc']) for h in histories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best in terms of final validation set accuracy\n",
    "i = best_scores.argmax()\n",
    "print('Hyperparameters: trial %i conv %s fc %s dropout %.3f opt %s, lr %.3f' %\n",
    "      (i, conv_sizes[i], fc_sizes[i], dropout[i], optimizer[i], lr[i]))\n",
    "print('  Last validation accuracy %.4f' % last_scores[i])\n",
    "print('  Best validation accuracy %.4f' % best_scores[i])\n",
    "draw_history(histories[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the worst in terms of final validation set accuracy\n",
    "i = best_scores.argmin()\n",
    "\n",
    "print('Hyperparameters: trial %i conv %s fc %s dropout %.3f opt %s, lr %.3f' %\n",
    "      (i, conv_sizes[i], fc_sizes[i], dropout[i], optimizer[i], lr[i]))\n",
    "print('  Last validation accuracy %.4f' % last_scores[i])\n",
    "print('  Best validation accuracy %.4f' % best_scores[i])\n",
    "draw_history(histories[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for trends in the best hyper-parameter sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(best_scores, bins=20, range=(0.5, 1))\n",
    "plt.xlabel('Validation accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in best_scores.argsort()[::-1][:5]:\n",
    "    print('Hyperparameters: trial %i conv %s fc %s dropout %.3f opt %s, lr %.3f' %\n",
    "          (i, conv_sizes[i], fc_sizes[i], dropout[i], optimizer[i], lr[i]))\n",
    "    print('  Best validation accuracy %.4f' % best_scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set evaluation\n",
    "\n",
    "Here we load the best selected model and evaluate final performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def summarize_metrics(labels, outputs, threshold=0.5, weights=None):\n",
    "    preds = outputs > threshold\n",
    "    #print('Metrics summaries with threshold of %.3f' % threshold)\n",
    "    print('Accuracy:   %.4f' % metrics.accuracy_score(labels, preds, sample_weight=weights))\n",
    "    print('Purity:     %.4f' % metrics.precision_score(labels, preds, sample_weight=weights))\n",
    "    print('Efficiency: %.4f' % metrics.recall_score(labels, preds, sample_weight=weights))\n",
    "\n",
    "def draw_roc(labels, outputs, weights=None, ax=None):\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, outputs, sample_weight=weights)\n",
    "    auc = metrics.roc_auc_score(labels, outputs, sample_weight=weights)\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.plot(fpr, tpr, label='CNN, AUC=%.3f' % auc)\n",
    "    ax.plot([0, 1], [0, 1], '--', label='Random')\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_input, test_labels, test_weights = load_file(os.path.join(input_dir, 'test.h5'), n_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = best_scores.argmax()\n",
    "model_file = os.path.join(checkpoint_dir, 'model_%i.h5' % i)\n",
    "model = keras.models.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model.predict(test_input)\n",
    "test_output = test_output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted results\n",
    "print('Unweighted metrics')\n",
    "summarize_metrics(test_labels, test_output)\n",
    "\n",
    "# Weighted results\n",
    "print('Weighted metrics')\n",
    "summarize_metrics(test_labels, test_output, weights=test_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(9,4))\n",
    "draw_roc(test_labels, test_output, ax=axs[0])\n",
    "draw_roc(test_labels, test_output, ax=axs[1], weights=test_weights)\n",
    "axs[0].set_xlim([0, 0.001])\n",
    "axs[0].set_title('Unweighted')\n",
    "axs[1].set_xlim([0, 0.001])\n",
    "axs[1].set_title('Weighted');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow-intel(cpu)/1.13.1-py36",
   "language": "python",
   "name": "tensorflow_intel_1.13.1_py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
