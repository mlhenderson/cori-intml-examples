{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distributed random-search hyper-parameter optimization of the Keras RPV classifier with interative widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System imports\n",
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "from __future__ import absolute_import\n",
    "import os\n",
    "from functools import partial\n",
    "\n",
    "# External imports\n",
    "import ipyparallel as ipp\n",
    "import numpy as np\n",
    "import keras\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Local imports\n",
    "from rpv import load_file\n",
    "from hpo_widgets import ModelPlot, ParamSpanWidget"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             JOBID PARTITION     NAME     USER ST       TIME  NODES NODELIST(REASON)\n",
      "          13275748      resv       sh sfarrell  R      26:43      8 nid0[1393-1400]\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "squeue -u sfarrell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Worker IDs: [0, 1, 2, 3, 4, 5, 6, 7]\n"
     ]
    }
   ],
   "source": [
    "# Cluster ID taken from job ID above\n",
    "job_id = 13275748\n",
    "cluster_id = 'cori_{}'.format(job_id)\n",
    "\n",
    "# Use default profile\n",
    "c = ipp.Client(timeout=60, cluster_id=cluster_id)\n",
    "print('Worker IDs:', c.ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the hyper-parameter search tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data config\n",
    "n_train = 64000 #412416\n",
    "n_valid = 32000 #137471\n",
    "n_test = 32000 #137471\n",
    "input_dir = '/global/cscratch1/sd/sfarrell/atlas-rpv-images'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporarily making things reproducible for development\n",
    "np.random.seed(0)\n",
    "\n",
    "# Define the hyper-parameter search points\n",
    "n_hpo_trials = 4 #48\n",
    "h1 = np.random.choice([4, 8, 16, 32, 64], size=n_hpo_trials)\n",
    "h2 = np.random.choice([4, 8, 16, 32, 64], size=n_hpo_trials)\n",
    "h3 = np.random.choice([8, 16, 32, 64, 128], size=n_hpo_trials)\n",
    "conv_sizes = np.stack([h1, h2, h3], axis=1)\n",
    "fc_sizes = np.random.choice([32, 64, 128, 256], size=(n_hpo_trials, 1))\n",
    "lr = np.random.choice([0.0001, 0.001, 0.01], size=n_hpo_trials)\n",
    "dropout = np.random.rand(n_hpo_trials)\n",
    "optimizer = np.random.choice(['Adadelta', 'Adam', 'Nadam'], size=n_hpo_trials)\n",
    "\n",
    "# Training config\n",
    "batch_size = 64\n",
    "n_epochs = 2 #16\n",
    "checkpoint_dir = '/global/cscratch1/sd/sfarrell/cori-interactive-dl/rpv_hpo_%i' % job_id\n",
    "os.makedirs(checkpoint_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the hyper-parameter search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_and_train(input_dir, n_train, n_valid,\n",
    "                    conv_sizes, fc_sizes, dropout, optimizer, lr,\n",
    "                    batch_size, n_epochs, checkpoint_file=None, verbose=2):\n",
    "    \"\"\"Run training for one set of hyper-parameters\"\"\"\n",
    "    import os\n",
    "    import keras\n",
    "    from rpv import build_model, train_model, load_file\n",
    "    from mlextras import configure_session, IPyParallelLogger\n",
    "    print('Hyperparameters: conv %s fc %s dropout %.3f opt %s, lr %.4f' %\n",
    "          (conv_sizes, fc_sizes, dropout, optimizer, lr))\n",
    "    # Load the dataset\n",
    "    train_input, train_labels, train_weights = load_file(os.path.join(input_dir, 'train.h5'), n_train)\n",
    "    valid_input, valid_labels, valid_weights = load_file(os.path.join(input_dir, 'val.h5'), n_valid)\n",
    "    print('train shape:', train_input.shape, 'Mean label:', train_labels.mean())\n",
    "    print('valid shape:', valid_input.shape, 'Mean label:', valid_labels.mean())\n",
    "    # Thread settings\n",
    "    keras.backend.set_session(configure_session())\n",
    "    # Build the model\n",
    "    model = build_model(train_input.shape[1:],\n",
    "                        conv_sizes=conv_sizes, fc_sizes=fc_sizes,\n",
    "                        dropout=dropout, optimizer=optimizer, lr=lr)\n",
    "    # Train the model\n",
    "    history = train_model(model, train_input=train_input, train_labels=train_labels,\n",
    "                          valid_input=valid_input, valid_labels=valid_labels,\n",
    "                          batch_size=batch_size, n_epochs=n_epochs,\n",
    "                          checkpoint_file=checkpoint_file, verbose=verbose, callbacks=[IPyParallelLogger()])\n",
    "    return history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8dc1dba7b12f453fae791b31b4c779c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "ParamSpanWidget(children=(Output(layout=Layout(border='1px solid', height='600px', overflow_x='scroll', overflâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_func = partial(build_and_train, input_dir=input_dir,\n",
    "                     n_train=n_train, n_valid=n_valid,\n",
    "                     batch_size=batch_size, n_epochs=n_epochs,\n",
    "                     verbose=2)\n",
    "\n",
    "plot_func = partial(\n",
    "    ModelPlot,\n",
    "    y=['loss', 'acc', 'val_loss', 'val_acc'],\n",
    "    xlim=[0, n_epochs],\n",
    "    xlabel='epochs',\n",
    "    ylabel='training metrics'\n",
    ")\n",
    "\n",
    "hpo_params = dict(\n",
    "    conv_sizes=conv_sizes,\n",
    "    fc_sizes=fc_sizes,\n",
    "    dropout=dropout,\n",
    "    optimizer=optimizer,\n",
    "    lr=lr,\n",
    ")\n",
    "\n",
    "psw = ParamSpanWidget(\n",
    "    compute_func=train_func, \n",
    "    vis_func=plot_func, \n",
    "    params=hpo_params,\n",
    "    ipp_cluster_id=cluster_id\n",
    ")\n",
    "\n",
    "psw.submit_computations()\n",
    "psw"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Load-balanced view\n",
    "lv = c.load_balanced_view()\n",
    "\n",
    "# Loop over hyper-parameter sets\n",
    "results = []\n",
    "for ihp in range(n_hpo_trials):\n",
    "    print('Hyperparameter trial %i conv %s fc %s dropout %.4f opt %s, lr %.4f' %\n",
    "          (ihp, conv_sizes[ihp], fc_sizes[ihp], dropout[ihp], optimizer[ihp], lr[ihp]))\n",
    "    checkpoint_file = os.path.join(checkpoint_dir, 'model_%i.h5' % ihp)\n",
    "    result = lv.apply(build_and_train,\n",
    "                      input_dir, n_train, n_valid,\n",
    "                      conv_sizes=conv_sizes[ihp], fc_sizes=fc_sizes[ihp],\n",
    "                      dropout=dropout[ihp], optimizer=optimizer[ihp], lr=lr[ihp],\n",
    "                      batch_size=batch_size, n_epochs=n_epochs,\n",
    "                      checkpoint_file=checkpoint_file)\n",
    "    results.append(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Doesn't work beyond here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Tasks completed: %i / %i' % (np.sum([ar.ready() for ar in results]), len(results)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ar = results[0]\n",
    "print(ar.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model selection and evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_history(h):\n",
    "    plt.figure(figsize=(9,4))\n",
    "    # Loss\n",
    "    plt.subplot(121)\n",
    "    plt.plot(h['loss'], label='Training')\n",
    "    plt.plot(h['val_loss'], label='Validation')\n",
    "    plt.xlim(xmin=0, xmax=len(h['loss'])-1)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend(loc=0)\n",
    "    # Accuracy\n",
    "    plt.subplot(122)\n",
    "    plt.plot(h['acc'], label='Training')\n",
    "    plt.plot(h['val_acc'], label='Validation')\n",
    "    plt.xlim(xmin=0, xmax=len(h['loss'])-1)\n",
    "    plt.ylim((0, 1))\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend(loc=0)\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_scores = np.array([h['val_acc'][-1] for h in histories])\n",
    "best_scores = np.array([max(h['val_acc']) for h in histories])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best in terms of final validation set accuracy\n",
    "i = best_scores.argmax()\n",
    "print('Hyperparameters: trial %i conv %s fc %s dropout %.3f opt %s, lr %.3f' %\n",
    "      (i, conv_sizes[i], fc_sizes[i], dropout[i], optimizer[i], lr[i]))\n",
    "print('  Last validation accuracy %.4f' % last_scores[i])\n",
    "print('  Best validation accuracy %.4f' % best_scores[i])\n",
    "draw_history(histories[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the worst in terms of final validation set accuracy\n",
    "i = best_scores.argmin()\n",
    "print('Hyperparameters: trial %i conv %s fc %s dropout %.3f opt %s, lr %.3f' %\n",
    "      (i, conv_sizes[i], fc_sizes[i], dropout[i], optimizer[i], lr[i]))\n",
    "print('  Last validation accuracy %.4f' % last_scores[i])\n",
    "print('  Best validation accuracy %.4f' % best_scores[i])\n",
    "draw_history(histories[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking for trends in the best hyper-parameter sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.hist(best_scores, bins=20, range=(0.5, 1))\n",
    "plt.xlabel('Validation accuracy');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in best_scores.argsort()[::-1][:5]:\n",
    "    print('Hyperparameters: trial %i conv %s fc %s dropout %.3f opt %s, lr %.3f' %\n",
    "          (i, conv_sizes[i], fc_sizes[i], dropout[i], optimizer[i], lr[i]))\n",
    "    print('  Best validation accuracy %.4f' % best_scores[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test set evaluation\n",
    "\n",
    "Here we load the best selected model and evaluate final performance on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "def summarize_metrics(labels, outputs, threshold=0.5, weights=None):\n",
    "    preds = outputs > threshold\n",
    "    #print('Metrics summaries with threshold of %.3f' % threshold)\n",
    "    print('Accuracy:   %.4f' % metrics.accuracy_score(labels, preds, sample_weight=weights))\n",
    "    print('Purity:     %.4f' % metrics.precision_score(labels, preds, sample_weight=weights))\n",
    "    print('Efficiency: %.4f' % metrics.recall_score(labels, preds, sample_weight=weights))\n",
    "\n",
    "def draw_roc(labels, outputs, weights=None, ax=None):\n",
    "    fpr, tpr, _ = metrics.roc_curve(labels, outputs, sample_weight=weights)\n",
    "    auc = metrics.roc_auc_score(labels, outputs, sample_weight=weights)\n",
    "    if ax is None:\n",
    "        fig, ax = plt.subplots()\n",
    "    ax.plot(fpr, tpr, label='CNN, AUC=%.3f' % auc)\n",
    "    ax.plot([0, 1], [0, 1], '--', label='Random')\n",
    "    ax.set_xlabel('False positive rate')\n",
    "    ax.set_ylabel('True positive rate')\n",
    "    ax.legend(loc=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = best_scores.argmax()\n",
    "model_file = os.path.join(checkpoint_dir, 'model_%i.h5' % i)\n",
    "model = keras.models.load_model(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_output = model.predict(test_input)\n",
    "test_output = test_output.squeeze(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unweighted results\n",
    "print('Unweighted metrics')\n",
    "summarize_metrics(test_labels, test_output)\n",
    "print()\n",
    "\n",
    "# Weighted results\n",
    "print('Weighted metrics')\n",
    "summarize_metrics(test_labels, test_output, weights=test_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(1, 2, figsize=(9,4))\n",
    "draw_roc(test_labels, test_output, ax=axs[0])\n",
    "draw_roc(test_labels, test_output, ax=axs[1], weights=test_weights)\n",
    "axs[0].set_xlim([0, 0.001])\n",
    "axs[0].set_title('Unweighted')\n",
    "axs[1].set_xlim([0, 0.001])\n",
    "axs[1].set_title('Weighted');"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "isc-ihpc",
   "language": "python",
   "name": "isc-ihpc"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
